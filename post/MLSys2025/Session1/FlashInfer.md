---
title: 'FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving'
date: 2025-11-21
permalink: /MLSys2025/Session1/FlashInfer/
toc: true
toc_sticky: true
tags:
  - MLSys2025
  - LLMs
---

including paper: [FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving](https://mlsys.org/virtual/2025/poster/3259)

## 问题与挑战

1. **工作负载多样性与输入动态性**：LLM服务包含多种计算模式（如预填充、解码），请求的序列长度变化大，存在前缀共享、树状解码等复杂模式，导致负载不均衡。
2. **硬件实现需要定制化**：高效的KV缓存存储格式（如分页注意力）和针对特定GPU架构的计算流水线至关重要，同时还需要支持不断涌现的注意力变体（如分组查询注意力、滑动窗口注意力等）。

## 解决方法

### **3.1 KV-Cache 存储**

#### **3.1.1 块稀疏矩阵作为统一格式**

**核心思想：** 将各种异构的KV-Cache存储格式，统一抽象为**块稀疏行（Block-Sparse Row， BSR）矩阵**。

- **背景：** 现有的高效KV-Cache管理技术，如 `PageAttention`（vLLM）和 `RadixAttention`（SGLang），都使用非连续的内存存储，其最小粒度是一个“块”（Block），每个块包含一个或多个token的Key和Value向量（形状为 `[H, D]`，H是头数，D是隐藏维度）。
- **统一抽象：** FlashInfer 认为，这些不同的数据结构（页表、基数树）都可以用一个BSR矩阵来表示。
  - **行（Rows）** 对应不同的查询。
  - **列（Columns）** 对应KV-Cache中的块。
  - **非零块（Non-zero blocks）** 表示该查询可以访问的KV-Cache块。
- **具体实现：**
  - **查询和输出** 被存储为**不规则张量（Ragged Tensors）**，无需填充，紧凑地打包不同请求的数据。
  - **Keys和Values** 最初也以不规则张量形式存在，随后被更新到KV-Cache中。
  - **KV-Cache** 使用BSR格式，其块大小 `(B_r, B_c)` 由应用需求决定：
    - `B_r`（行块大小）：与查询的Tile大小对齐（在计算部分详述）。
    - `B_c`（列块大小）：由KV-Cache管理算法指定（例如，一个物理块包含的token数）。
  - **优势：** 这种抽象使得一个内核就能处理来自不同服务系统（vLLM， SGLang）的KV-Cache布局，甚至能表示树状注意力等复杂模式。

**图2** 直观展示了如何将一个页表表示为BSR格式 `(B_r=4, B_c=1)`。

#### **3.1.2 可组合格式以实现内存效率**

**核心思想：** 使用**多个**具有不同块大小的BSR矩阵来存储KV-Cache，而不是单一的固定格式，以优化内存访问。

- **单一格式的局限性：**
  - **大的 `B_r`**： 好处是同一个块内的多个查询可以共享KV-Cache数据，并利用高速的共享内存和寄存器。**坏处**是如果请求不共享前缀，会导致内存碎片化。
  - **小的 `B_r`**： 减少了碎片，但每个查询线程块只能通过低带宽的全局内存或L2缓存访问KV-Cache，效率低。
- **可组合格式的解决方案：**
  - 根据先验知识（如哪些请求共享一个前缀）对KV-Cache稀疏矩阵进行**分解**。
  - 共享前缀对应的行和列形成一个**稠密子矩阵**，可以用一个**较大 `B_r`** 的BSR矩阵来存储它。这样，在计算共享部分时，可以利用共享内存的高带宽。
  - 剩余的唯一部分则用一个**较小 `B_r`** 的BSR矩阵存储。
  - **关键：** 这不需要实际移动KV-Cache中的数据，只需计算不同稀疏子矩阵的索引和指针数组。

**图3** 展示了这一概念：前6个查询共享一个前缀，后6个查询共享另一个前缀。共享前缀的KV-Cache用 `(3,1)` 的块存储，而唯一部分用 `(1,1)` 的块存储。

------

### **3.2 计算抽象**

FlashInfer 基于 CUDA/CUTLASS 构建了FlashAttention的模板，支持从Turing到Hopper的GPU架构。

#### **3.2.1 全局内存到共享内存的数据搬运**

- **挑战：** 由于支持任意块大小，这些块可能与Tensor Core的指令形状（如16x16）不对齐。
- **解决方案：**
  1. **聚集（Gather）**： 首先将分散在全局内存中的稀疏KV-Cache块（通过BSR的`indices`数组计算地址）或稠密KV-Cache块（通过仿射变换计算地址）**异步拷贝**到**连续的共享内存**中。
  2. **密集计算**： 一旦数据在共享内存中连续，就可以使用标准的、高效的Tensor Core指令进行密集的矩阵乘加运算。
- **硬件优化：**
  - 使用 `LDGSTS` 异步拷贝指令（128B宽度）来最大化内存带宽。
  - 对于Hopper的TMA，它只支持连续的访存模式，因此FlashInfer仅在KV-Cache连续时使用TMA，否则回退到Ampere风格的异步拷贝。

**图4** 对比了稀疏和稠密KV-Cache的数据加载过程。

#### **3.2.2 具有不同Tile大小的微内核**

- **背景：** 不同LLM应用场景（预填充 vs. 解码）的计算强度不同，固定的Tile大小（如 `(128,64)`）并非最优。
- **FlashInfer的解决方案：** 提供多种Tile大小配置 `(1,16,32,64,128) x (32,64,128)`，并通过启发式方法动态选择：
  1. **根据查询长度选择 `T_q`（行Tile大小）**： 计算批次中平均查询长度，选择能满足该长度的最小 `T_q`。
  2. **根据硬件资源选择 `T_kv`（KV Tile大小）**： 将寄存器和共享内存的消耗建模为 `T_kv` 的函数，选择能最大化流多处理器占用率的 `T_kv`。
- **架构适配：**
  - 对于 `T_q = 1`，使用CUDA核心（因为Tensor Core的最小行数是16）。
  - 对于Hopper和FA3，提供行Tile大小为64倍数的内核，以匹配 `WGMMA` 指令的要求。

------

### **3.3 JIT编译器用于注意力变体**

**核心思想：** 为了避免为每个注意力变体都手写一个高性能CUDA内核的维护噩梦，FlashInfer设计了一个**可定制的模板和一个JIT编译器**。

- **变体规范（Variant Specification）**： 用户通过定义一个包含多个“仿函数”的类来指定其注意力变体：
  - `QueryTransform`, `KeyTransform`, `ValueTransform`： 在注意力计算前对Q/K/V进行变换（例如，**融合LayerNorm、RoPE、投影**）。
  - `LogitsTransform`, `LogitsMask`： 在Softmax之前对注意力分数进行处理（例如，应用**滑动窗口掩码、soft cap**）。
  - `OutputTransform`： 对注意力输出进行变换。
  - **可选Softmax**： 支持不需要Softmax的变体，如FlashSigmoid。
- **JIT编译流程**：
  1. 用户提供定义了这些仿函数的CUDA代码片段。
  2. JIT编译器将用户的变体类和其他信息（如Tile大小）插入到预定义的CUDA模板中。
  3. 生成完整的、高度优化的CUDA代码。
  4. 使用PyTorch的JIT编译器将其编译为自定义算子，或通过DLPack接口集成到其他框架中。

**图5** 展示了如何将FlashSigmoid的规范映射到FlashInfer的CUDA模板中。

------

### **3.4 动态感知的运行时**

#### **3.4.1 负载均衡调度**

**目标：** 将工作量均匀分配到所有SM上，最小化空闲时间。

- **工作流程（图6和算法1）**：

  1. **输入**： 查询和KV序列的长度信息。
  2. **规划**： 在CPU上运行调度算法，生成两个关键信息：

  - **每个CTA的工作队列**： 指定每个线程块需要处理哪些查询和KV块。
  - **索引映射**： 指定如何将部分输出聚合到最终输出中。

  1. **执行**： 将规划信息异步拷贝到GPU的工作空间缓冲区。然后启动**持久化内核**，该内核根据规划信息执行注意力计算和部分结果的聚合。

- **关键技术点**：

  - **确定性**： 为保证LLM服务的确定性输出，未使用原子操作，调度算法对相同的输入总是生成相同的聚合顺序。
  - **与CUDA Graph兼容**：
    - 内核的网格大小在编译时固定。
    - 工作空间缓冲区的指针在每次生成步骤中都保持不变。
    - 将注意力和聚合阶段合并为一个持久化内核，消除了内核间启动开销。
    - 因此，整个 `run` 函数可以被CUDA Graph捕获和重放。

#### **3.5 编程接口**

FlashInfer提供了易于使用的PyTorch接口（见**代码清单1**），便于集成到现有框架中。

- **初始化**： `AttentionWrapper(attn_spec, task_info, workspace)`。在初始化时根据注意力规范和任务信息JIT编译内核。
- **运行时循环**：
  1. **规划（`plan`）**： 在每个生成步骤开始时，根据最新的序列长度信息调用此CPU函数，生成负载均衡计划。
  2. **执行（`run`）**： 被CUDA Graph捕获的函数，执行实际的注意力计算。
- **可组合格式的支持**： 对于可组合格式，用户创建多个AttentionWrapper实例，每个对应不同的块大小。在运行时，服务框架选择最适合当前KV-Cache配置的实例（及其对应的CUDA Graph）来执行。

## 知识点

### 预填充（prefill）

**预填充阶段** 是指在处理一个完整的输入序列（例如一个句子或一个问题）时，模型在生成第一个输出词元（token）**之前**，一次性、并行地计算并缓存整个输入序列的键（Key）和值（Value）向量的过程。

其主要特征是：**计算密集，GPU 利用率高**

### 解码阶段（decode）

**解码阶段** 是指在预填充阶段完成后，模型以**自回归**的方式，逐个生成输出词元的过程。每一个新词元的生成，都依赖于输入序列和之前已经生成的所有词元。

其主要特征是：**内存密集，GPU 利用率低，但可通过批处理提高吞吐量**

### KV Cache

KV Cache 的核心思想是：**避免重复计算**。

- **缓存什么？**
  - 在计算第 `i` 个 token 时，每个解码层都会为该 token 计算出一套 Key 向量和 Value 向量。
  - 当计算完成，准备生成下一个 token `i+1` 时，我们**不释放**第 `i` 个 token 的 K 和 V。
  - 相反，我们将它们存储起来。这个存储空间就是 **KV Cache**。
- **如何使用？**
  - 在生成第 `t+1` 个 token 时：
    1. 我们只需要为这个**新的** token 计算其 `Q_{t+1}`, `K_{t+1}`, `V_{t+1}`。
    2. 对于之前 `t` 个 tokens 的 `K` 和 `V`，我们直接从 KV Cache 中读取，无需重新计算。
    3. 将新的 `K_{t+1}` 和 `V_{t+1}` **追加**到 Cache 中，为下一次生成做准备。

### FlashAttention

传统自注意力机制的计算复杂度随序列长度呈平方级增长，成为处理长序列的主要瓶颈。FlashAttention 的创新在于其 **IO 感知** 的设计思路，它重点关注如何更高效地利用GPU的内存层次结构，特别是减少在高速缓存（SRAM）和高带宽内存（HBM）之间的频繁数据读写。

为了实现这一目标，FlashAttention 主要采用了三种关键技术：

- **Tiling（分块）**：将大的输入矩阵（Q, K, V）分割成较小的块，将这些块从HBM加载到更快的SRAM中进行计算。由于SRAM容量有限，计算需要通过多次循环完成。
- **重计算**：在前向传播过程中，FlashAttention **不存储** 庞大的中间注意力矩阵（即Q和K相乘后的结果）。在反向传播需要梯度时，它会利用存储的归一化因子等少量信息和输入矩阵（Q, K, V）来 **重新计算** 注意力矩阵。这巧妙地用额外的计算开销换取了巨大的内存节省。
- **Kernel Fusion**：将自注意力计算中的多个步骤（如矩阵乘法、Softmax和掩码应用）融合到单个CU核中执行。这样可以避免每个中间步骤都将结果写回HBM，减少了整体内存访问次数。

### 不规则张量（Ragged Tensors）

**Ragged Tensor（不规则张量）**，也称为 **Jagged Tensor（锯齿状张量）**，是一种特殊的张量，其一个或多个维度内的元素长度是可变的。

Ragged Tensor 可以看作是一个**嵌套的可变长度列表**。其核心思想是使用两个部分来定义：

1. **Values（值）**：一个扁平化的 1D 张量，将所有序列的所有元素连接在一起。
2. **Ragged Dimensions（不规则维度）**：通过一个或多个 **“行分割”数组** 来定义如何将 `Values` 分割成不同长度的行。

**例子：**

- 句子: `[ ["Hello"], ["How", "are", "you"], ["Good", "morning"] ]`
- **Values**: `["Hello", "How", "are", "you", "Good", "morning"]`
- **Row Splits**: `[0, 1, 4, 6]`
  - 这个分割数组的含义是：
    - 第0行：从索引0到1 (`1-0=1` 个元素)
    - 第1行：从索引1到4 (`4-1=3` 个元素)
    - 第2行：从索引4到6 (`6-4=2` 个元素)

在 PyTorch、TensorFlow 或 JAX 中，这个结构被封装起来，你可以像一个列表的列表一样去操作它，而底层由高效的 `Values` 和 `Row Splits` 表示。

优点：

1. **无计算浪费**：避免了在填充位置上执行无用的计算（如矩阵乘法、卷积等），节省了计算资源。
2. **无内存浪费**：不存储填充符，节省了内存。
3. **更清晰的逻辑**：无需在处理过程中一直携带和传播掩码，代码逻辑更简洁，不易出错。
4. **动态序列的自然表示**：非常适合处理如文本、DNA序列、图结构（邻接列表）等天生不规则的数据。

### 块稀疏注意力（Block/Vector Sparsity）

**块稀疏注意力的基本假设是：** 在一个序列中，并不是所有 token 对之间都存在重要的关联。例如，在理解一段文本时，一个词通常只与它周围的词以及少数几个关键 distant tokens 有强关联。

因此，块稀疏注意力**只计算所有可能 token 对中的一个子集**，而忽略其他不重要的连接，从而实现加速和节省内存。

- **块**：将原始的注意力矩阵（QK^T）在逻辑上划分为多个较小的、连续的方块。每个方块代表两个 token 块之间的所有可能的注意力连接。
- **稀疏模式**：我们不是决定“单个 token A 是否需要注意 token B”，而是决定“**块 A 中的所有 token 是否需要注意块 B 中的所有 token**”。

1. **划分块**：将查询（Q）、键（K）、值（V）矩阵在序列长度维度上划分为多个块。
2. **确定稀疏模式**：根据预定义规则或某种轻量计算，决定哪些块对之间需要计算注意力，哪些可以跳过。常见的模式包括：
   - **局部注意力**：一个 token 块只关注其附近的几个块（类似于卷积的局部感受野）。
   - **全局注意力**：保留少数重要的“全局”token（如 [CLS] token 或段落的开头 token），让所有块都关注它们。
   - **随机注意力**：随机选择一些块进行连接。
   - **带状/滑动窗口注意力**：固定一个窗口，每个块只关注窗口内的其他块。
3. **选择性计算**：只计算被选中的块对之间的注意力。对于未被选中的块，直接跳过整个矩阵乘法和 Softmax 计算。
4. **集成 FlashAttention 技术**：对于这些需要计算的块对，使用高度优化的 **FlashAttention 内核** 来进行计算。这使得每个块内的计算达到硬件层面的最高效率。

### 计算密度（operational intensity）

**操作强度 = 总执行的操作数 / 总的数据搬运量**

- **总执行的操作数**：通常指的是浮点数运算（FLOP）的总次数，例如在矩阵乘法、卷积等计算中。
- **总的数据搬运量**：指的是在计算过程中，在内存层次结构（例如，从慢速的主存/高带宽内存HBM到高速的缓存Cache）之间移动的数据总量，通常以字节为单位。

**简单来说，操作强度回答了一个问题：“每从内存中搬运1个字节的数据，能完成多少次有用的计算？”**

#### 罗夫定律

操作强度的重要性可以通过一个经典的模型——**罗夫模型** 来理解。该模型指出，一个程序的**实际性能** 由两个因素共同决定：

1. **算力**：你的硬件（如CPU/GPU）每秒能执行多少次运算。这是硬件的**速度上限**。
2. **带宽**：你的内存系统每秒能传输多少数据。这是数据的**供应上限**。

而连接这两者的桥梁，正是**操作强度**。

- **算力瓶颈**：如果你的操作强度很高，意味着你每搬一次数据就能做大量的计算。那么程序的性能瓶颈在于你的硬件能算得多快。这被称为 **Compute-Bound**。
- **带宽瓶颈**：如果你的操作强度很低，意味着你每做一次计算就需要搬很多次数据。那么程序的性能瓶颈在于你的内存能供应数据多快。这被称为 **Memory-Bound**。
