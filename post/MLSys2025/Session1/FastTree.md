---
title: 'FastTree: Optimizing Attention Kernel and Runtime for Tree-Structured LLM Inference'
date: 2025-11-21
permalink: /MLSys2025/Session1/FastTree/
toc: true
toc_sticky: true
tags:
  - MLSys2025
  - LLMs
---

including paper: [FastTree: Optimizing Attention Kernel and Runtime for Tree-Structured LLM Inference](https://mlsys.org/virtual/2025/poster/3278)

## 问题与挑战

1. **工作负载多样性与输入动态性**：LLM服务包含多种计算模式（如预填充、解码），请求的序列长度变化大，存在前缀共享、树状解码等复杂模式，导致负载不均衡。
2. **硬件实现需要定制化**：高效的KV缓存存储格式（如分页注意力）和针对特定GPU架构的计算流水线至关重要，同时还需要支持不断涌现的注意力变体（如分组查询注意力、滑动窗口注意力等）。

## 解决方法

### **3.1 KV-Cache 存储**

#### **3.1.1 块稀疏矩阵作为统一格式**

**核心思想：** 将各种异构的KV-Cache存储格式，统一抽象为**块稀疏行（Block-Sparse Row， BSR）矩阵**。

- **背景：** 现有的高效KV-Cache管理技术，如 `PageAttention`（vLLM）和 `RadixAttention`（SGLang），都使用非连续的内存存储，其最小粒度是一个“块”（Block），每个块包含一个或多个token的Key和Value向量（形状为 `[H, D]`，H是头数，D是隐藏维度）。
- **统一抽象：** FlashInfer 认为，这些不同的数据结构（页表、基数树）都可以用一个BSR矩阵来表示。
  - **行（Rows）** 对应不同的查询。
  - **列（Columns）** 对应KV-Cache中的块。
  - **非零块（Non-zero blocks）** 表示该查询可以访问的KV-Cache块。
- **具体实现：**
  - **查询和输出** 被存储为**不规则张量（Ragged Tensors）**，无需填充，紧凑地打包不同请求的数据。
  - **Keys和Values** 最初也以不规则张量形式存在，随后被更新到KV-Cache中。
  - **KV-Cache** 使用BSR格式，其块大小 `(B_r, B_c)` 由应用需求决定：
    - `B_r`（行块大小）：与查询的Tile大小对齐（在计算部分详述）。
    - `B_c`（列块大小）：由KV-Cache管理算法指定（例如，一个物理块包含的token数）。
  - **优势：** 这种抽象使得一个内核就能处理来自不同服务系统（vLLM， SGLang）的KV-Cache布局，甚至能表示树状注意力等复杂模式。

**图2** 直观展示了如何将一个页表表示为BSR格式 `(B_r=4, B_c=1)`。

#### **3.1.2 可组合格式以实现内存效率**

**核心思想：** 使用**多个**具有不同块大小的BSR矩阵来存储KV-Cache，而不是单一的固定格式，以优化内存访问。

- **单一格式的局限性：**
  - **大的 `B_r`**： 好处是同一个块内的多个查询可以共享KV-Cache数据，并利用高速的共享内存和寄存器。**坏处**是如果请求不共享前缀，会导致内存碎片化。
  - **小的 `B_r`**： 减少了碎片，但每个查询线程块只能通过低带宽的全局内存或L2缓存访问KV-Cache，效率低。
- **可组合格式的解决方案：**
  - 根据先验知识（如哪些请求共享一个前缀）对KV-Cache稀疏矩阵进行**分解**。
  - 共享前缀对应的行和列形成一个**稠密子矩阵**，可以用一个**较大 `B_r`** 的BSR矩阵来存储它。这样，在计算共享部分时，可以利用共享内存的高带宽。
  - 剩余的唯一部分则用一个**较小 `B_r`** 的BSR矩阵存储。
  - **关键：** 这不需要实际移动KV-Cache中的数据，只需计算不同稀疏子矩阵的索引和指针数组。

**图3** 展示了这一概念：前6个查询共享一个前缀，后6个查询共享另一个前缀。共享前缀的KV-Cache用 `(3,1)` 的块存储，而唯一部分用 `(1,1)` 的块存储。

------

### **3.2 计算抽象**

FlashInfer 基于 CUDA/CUTLASS 构建了FlashAttention的模板，支持从Turing到Hopper的GPU架构。

#### **3.2.1 全局内存到共享内存的数据搬运**

- **挑战：** 由于支持任意块大小，这些块可能与Tensor Core的指令形状（如16x16）不对齐。
- **解决方案：**
  1. **聚集（Gather）**： 首先将分散在全局内存中的稀疏KV-Cache块（通过BSR的`indices`数组计算地址）或稠密KV-Cache块（通过仿射变换计算地址）**异步拷贝**到**连续的共享内存**中。
  2. **密集计算**： 一旦数据在共享内存中连续，就可以使用标准的、高效的Tensor Core指令进行密集的矩阵乘加运算。
- **硬件优化：**
  - 使用 `LDGSTS` 异步拷贝指令（128B宽度）来最大化内存带宽。
  - 对于Hopper的TMA，它只支持连续的访存模式，因此FlashInfer仅在KV-Cache连续时使用TMA，否则回退到Ampere风格的异步拷贝。

**图4** 对比了稀疏和稠密KV-Cache的数据加载过程。

#### **3.2.2 具有不同Tile大小的微内核**

- **背景：** 不同LLM应用场景（预填充 vs. 解码）的计算强度不同，固定的Tile大小（如 `(128,64)`）并非最优。
- **FlashInfer的解决方案：** 提供多种Tile大小配置 `(1,16,32,64,128) x (32,64,128)`，并通过启发式方法动态选择：
  1. **根据查询长度选择 `T_q`（行Tile大小）**： 计算批次中平均查询长度，选择能满足该长度的最小 `T_q`。
  2. **根据硬件资源选择 `T_kv`（KV Tile大小）**： 将寄存器和共享内存的消耗建模为 `T_kv` 的函数，选择能最大化流多处理器占用率的 `T_kv`。
- **架构适配：**
  - 对于 `T_q = 1`，使用CUDA核心（因为Tensor Core的最小行数是16）。
  - 对于Hopper和FA3，提供行Tile大小为64倍数的内核，以匹配 `WGMMA` 指令的要求。

------

### **3.3 JIT编译器用于注意力变体**

**核心思想：** 为了避免为每个注意力变体都手写一个高性能CUDA内核的维护噩梦，FlashInfer设计了一个**可定制的模板和一个JIT编译器**。

- **变体规范（Variant Specification）**： 用户通过定义一个包含多个“仿函数”的类来指定其注意力变体：
  - `QueryTransform`, `KeyTransform`, `ValueTransform`： 在注意力计算前对Q/K/V进行变换（例如，**融合LayerNorm、RoPE、投影**）。
  - `LogitsTransform`, `LogitsMask`： 在Softmax之前对注意力分数进行处理（例如，应用**滑动窗口掩码、soft cap**）。
  - `OutputTransform`： 对注意力输出进行变换。
  - **可选Softmax**： 支持不需要Softmax的变体，如FlashSigmoid。
- **JIT编译流程**：
  1. 用户提供定义了这些仿函数的CUDA代码片段。
  2. JIT编译器将用户的变体类和其他信息（如Tile大小）插入到预定义的CUDA模板中。
  3. 生成完整的、高度优化的CUDA代码。
  4. 使用PyTorch的JIT编译器将其编译为自定义算子，或通过DLPack接口集成到其他框架中。

**图5** 展示了如何将FlashSigmoid的规范映射到FlashInfer的CUDA模板中。

------

### **3.4 动态感知的运行时**

#### **3.4.1 负载均衡调度**

**目标：** 将工作量均匀分配到所有SM上，最小化空闲时间。

- **工作流程（图6和算法1）**：

  1. **输入**： 查询和KV序列的长度信息。
  2. **规划**： 在CPU上运行调度算法，生成两个关键信息：

  - **每个CTA的工作队列**： 指定每个线程块需要处理哪些查询和KV块。
  - **索引映射**： 指定如何将部分输出聚合到最终输出中。

  1. **执行**： 将规划信息异步拷贝到GPU的工作空间缓冲区。然后启动**持久化内核**，该内核根据规划信息执行注意力计算和部分结果的聚合。

- **关键技术点**：

  - **确定性**： 为保证LLM服务的确定性输出，未使用原子操作，调度算法对相同的输入总是生成相同的聚合顺序。
  - **与CUDA Graph兼容**：
    - 内核的网格大小在编译时固定。
    - 工作空间缓冲区的指针在每次生成步骤中都保持不变。
    - 将注意力和聚合阶段合并为一个持久化内核，消除了内核间启动开销。
    - 因此，整个 `run` 函数可以被CUDA Graph捕获和重放。

#### **3.5 编程接口**

FlashInfer提供了易于使用的PyTorch接口（见**代码清单1**），便于集成到现有框架中。

- **初始化**： `AttentionWrapper(attn_spec, task_info, workspace)`。在初始化时根据注意力规范和任务信息JIT编译内核。
- **运行时循环**：
  1. **规划（`plan`）**： 在每个生成步骤开始时，根据最新的序列长度信息调用此CPU函数，生成负载均衡计划。
  2. **执行（`run`）**： 被CUDA Graph捕获的函数，执行实际的注意力计算。
- **可组合格式的支持**： 对于可组合格式，用户创建多个AttentionWrapper实例，每个对应不同的块大小。在运行时，服务框架选择最适合当前KV-Cache配置的实例（及其对应的CUDA Graph）来执行。

## 知识点

### 基数树（Radix Tree）

- **基本概念**：Radix树源于前缀树，通过**合并只有一个子节点的节点与其父节点**来节省空间，是一种**压缩后的前缀树**。例如，存储IP地址时，Radix树可以高效处理像 `192.168.1.0/24` 这样的可变长度网络前缀。
- **与相关结构的区别**
  - **与前缀树的区别**：普通前缀树为每个字符创建节点，可能浪费空间。Radix树通过**压缩单分支节点**优化了这一点。
  - **与Patricia树的关系**：这两个术语经常混用。狭义上，**Patricia树是Radix树的一种特例，其基数为2（即按位比较和分支）**。广义上，许多资料将两者视为同一概念，即**压缩前缀树**。（如：将`key`转为二进制`01101011 01100101 01111001`然后以01来分叉。）

<img src="../assets/post/FastTree-Trie.png" alt="Trie" style="zoom:50%;" />

<img src="../assets/post/FastTree-radix-tree.png" alt="Radix Tree" style="zoom:50%;" />

### 长尾效应（long tail effect）

#### **动态输入长度不均等**

- **场景**：一批处理（Batch）中同时处理多个用户请求。
- **问题**：每个请求的输入提示（Prompt）长度和需要生成的输出（Output）长度差异巨大。
- **例子**：一批处理32个请求。其中31个请求生成了10个令牌就完成了，但第32个请求需要生成200个令牌。系统必须等待这个最慢的请求完成，才能释放资源处理下一批。这个**200令牌的请求就是“长尾”**。
