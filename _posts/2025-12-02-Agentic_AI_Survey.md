---
title: 'Agentic AI Survey'
date: 2025-12-02
permalink: #/posts/
tags:
  - Agentic AI
---

更新相关研究现状（粗略信息）。

# EuroSys '25

## Towards VM Rescheduling Optimization Through Deep Reinforcement Learning

使用强化学习对整个服务器集群内部的VM进行重调度以减少碎片化。该重调度选在每天的闲时或者当前服务器状态无法满足一个新任务时。

## Samoyeds: Accelerating MoE Models with Structured Sparsity Leveraging Sparse Tensor Cores 

### 一、研究背景与挑战

大语言模型已成为深度学习的核心范式。然而，随着模型规模的急剧扩张，尤其是参数量的爆炸式增长（如BERT-base的1.1亿参数 vs LLaMA3的4000亿参数），现有的AI基础设施面临着巨大的**计算与内存挑战**。硬件发展速度的有限性使得部署这些超大模型变得异常困难。

在这一背景下，**混合专家模型（Mixture-of-Experts, MoE）** 作为一种新型架构被广泛采用。MoE通过“选择性激活”专家来提升模型的泛化能力和多任务处理能力，但其**多层专家结构**也带来了独特的**存储、带宽和计算资源需求**，进一步加剧了部署难度。

### 二、现有方案的局限性

为了解决上述挑战，**稀疏计算**被视为一种有前景的技术路径。但传统**非结构化稀疏**（如COO、CSR格式）由于非零元素的**不规则模式**，在现代GPU上难以实现高效并行，无法充分利用如**合并内存访问**等关键硬件特性。

因此，**结构化稀疏**应运而生。它通过规则化的稀疏模式（如N:M模式）来提高硬件效率，并得到了NVIDIA **稀疏张量核心（Sparse Tensor Cores, SpTC）** 的硬件支持（自Ampere架构起，性能可达稠密核心的2倍）。这为实现**软硬件协同设计**以加速LLMs提供了可能。

然而，当前结构化稀疏的研究和实践存在**两大关键缺陷**：

1. **只关注模型参数的稀疏性，忽略了激活中的稀疏模式**。MoE层的路由机制天然会导致**输入的稀疏性**，而现有方法未能有效利用这一点，错失了进一步的优化机会。
2. **现有方案（如cuSPARSELt, VENOM）主要面向稀疏-稠密矩阵乘法**。当权重和输入**同时为稀疏**时，会面临**内存访问不连续**和**I/O放大**的问题，导致性能下降。  

### 三、研究动机与问题定位

基于以上分析，论文明确指出：

> **MoE LLMs的计算同时存在权重和输入的双重稀疏性，而现有的稀疏计算方案无法高效处理这种“双端稀疏”场景，从而成为性能瓶颈。**

因此，需要一种**新的稀疏数据格式**和**专用的计算内核**，以：

- 同时利用权重和输入的稀疏性
- 避免无效的内存访问和I/O开销
- 兼容SpTC硬件以实现最大加速

# ASPLOS '25

## FSMoE: A Flexible and Scalable Training System for Sparse Mixture-of-Experts Models

### 一、研究背景

- **背景**：近年来，稀疏激活的MoE层在大语言模型中受到广泛关注。
- **核心价值**：MoE可以在**几乎线性增加计算成本的情况下，大幅扩展模型规模**。

- **MoE层结构**：多个专家（FFN） + 门控路由机制。
- **两个层面的挑战**：
  - **算法层面**：如何有效训练专家、如何设计路由函数仍是开放问题。
  - **系统层面**：需要专门系统支持算法研究与模型部署。
- **现有系统优化方向**：高效通信算法、稀疏计算优化、任务调度等。

### 二、现有系统的局限性

1. **灵活性不足**：仅支持有限路由函数，难以扩展。
2. **性能非最优**：针对特定并行策略优化，在常见混合并行场景中表现不佳。

3. 混合并行的通信瓶颈
   - **并行范式**：除了传统的DP、MP、PP，MoE引入EP（专家并行）和ESP（专家分片并行）。
   - **通信开销**：AlltoAll通信占MoE层执行时间的30%~60%，在DP+MP+EP+ESP混合并行下更为严重。
   - **问题总结**：路由机制快速演进 + 并行策略复杂 → 系统难以灵活支持新组件，且任务调度面临性能挑战。

### 三、具体挑战

1. **灵活的MoE框架需求**：应支持多种路由函数、排序函数、专家块和通信算法，易于扩展。
2. **网络通信优化空间**：现有系统未能重叠节点内与节点间通信（如ESP-AllGather与AlltoAll）。
3. **前向与反向传播差异化调度**：二者计算需求不同，应有不同的流水线度。
4. **反向传播与梯度同步的协同设计**：Gradient-AllReduce与MoE层通信之间存在竞争，需协同调度。

### 四、本文工作与贡献

为解决上述问题，作者提出了 **FSMoE**（Flexible and Scalable MoE），一个模块化、可扩展、支持智能调度的 MoE 训练系统。其核心贡献包括：

1. **模块化设计与统一抽象**

- 将 MoE 层拆分为六个独立模块：**Gate、Order、I-Order、Dispatch、Combine、Expert**。
- 支持多种路由函数（GShard、Sigmoid、X-MoE、SoftMoE）、排序函数（GShard 风格、Tutel 风格）、通信算法（NCCL-A2A、1DH-A2A、2DH-A2A）和专家结构（GPT-FFN、Mixtral-FFN）。
- 提供**非侵入式钩子（hooks）**，允许用户在不修改核心代码的情况下扩展功能（如通信压缩、多模态数据处理）。

2. **智能任务调度算法**

- **节点内与节点间通信重叠**：提出一种调度策略，将节点内通信（ESP-AllGather/ESP-ReduceScatter）与节点间通信（AlltoAll Dispatch/Combine）流水线化，显著减少通信等待时间。
- **自适应流水线度选择**：基于性能模型，为前向传播和反向传播分别选择最优流水线度，适应二者不同的计算特性。
- **梯度分区分割与协同调度**：提出一种自适应梯度分割方法，将梯度同步（Gradient-AllReduce）与 MoE 层计算/通信重叠，避免通信竞争。

3. **性能建模与优化求解**

- 建立**线性性能模型**，对 GEMM 计算、AlltoAll、AllGather、ReduceScatter、AllReduce 等操作进行建模。
- 将调度问题形式化为**约束优化问题**，提出四类典型场景（Case1~Case4）并分别求解最优流水线度。
- 使用**差分进化算法**优化梯度分割策略，实现全局最优调度。

4. **系统实现与易用性**

- 基于 PyTorch 实现，支持 C++/CUDA 扩展。
- 提供简洁的 API，用户可通过继承抽象类或直接调用预定义模块快速构建 MoE 层。
- 支持混合并行（DP+MP+EP+ESP+PP），适用于大规模多 GPU 集群。

## MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts Model Training

### 一、研究背景与趋势

1. **大语言模型（LLMs）的崛起**
   - Transformer 架构的 LLMs 参数规模已达千亿乃至万亿级别，成为 AI 研究热点。
   - 模型规模的扩大带来了计算、存储和通信上的巨大挑战。
2. **稀疏混合专家模型（MoE）的兴起**
   - MoE 通过稀疏激活机制，在增加参数量的同时不显著增加计算量，成为扩展模型规模的主流方法。
   - 分布式训练系统为 MoE 模型引入了专家并行（EP），与数据并行（DP）、模型并行（MP）结合形成混合并行策略。
3. **故障容忍成为系统关键问题**
   - 分布式训练系统规模已超 10,000 节点，故障发生率随之上升。
   - 检查点（Checkpoint）是当前主流的容错机制，但其效率直接影响训练成本和系统可靠性。

### 二、问题提出：MoE 模型带来的新挑战

1. **检查点规模爆炸式增长**
   - MoE 模型参数量远超稠密模型，导致检查点大小急剧增加。
   - 分布式文件系统难以高效处理如此大规模的检查点写入与存储。
2. **检查点时间与训练过程难以重叠**
   - 即使使用异步检查点和分片技术，MoE 模型的检查点时间仍可能超过训练的前向与反向传播时间，导致训练停滞。
3. **现有方法缺乏对 MoE 特性的针对性优化**
   - 传统检查点策略未考虑 MoE 的结构稀疏性和专家分布特性。
   - 缺乏针对 MoE 模型的高效分片、异步和部分检查点机制。

### 三、解决方案：MoC-System 概述

论文提出 **Mixture-of-Checkpoint System（MoC-System）**，核心思想是**系统与算法协同设计**，通过多层次优化实现高效容错：

1. **提出 MoC-System（Mixture-of-Checkpoint System）**
   - 一个面向 MoE 模型训练的高效容错系统，集成了多种检查点管理与优化策略。
2. **提出 Partial Experts Checkpointing（PEC）机制**
   - 在每个检查点只保存部分专家参数，而非全部，大幅减小检查点大小。
   - 提出 **PLT（Proportion of Lost Tokens）** 指标，量化因部分保存而丢失的更新对精度的影响。
3. **设计 Fully Sharded Checkpointing 策略**
   - 对专家和非专家部分分别进行分片，实现跨分布式节点的负载均衡。
   - 提出自适应分片策略，根据 PEC 选择模式动态调整非专家参数的分片方式。
4. **实现 Two-Level Checkpointing Management**
   - 引入“内存快照 + 持久化存储”两级检查点管理机制，支持异步保存与恢复。
   - 提出 Triple Buffering 技术，支持异步检查点与训练的充分重叠。
5. **提出动态调整机制（Dynamic-K）**
   - 根据累积故障次数动态调整保存的专家数量，控制 PLT 不超过阈值，平衡效率与精度。

## CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with Limited Memory

### 一、研究背景

- **起点**：以 GPT-4 为代表的**大模型（Monolithic Models）** 虽然强大，但训练和调优成本极高。
- **转折**：近期研究表明，**规模较小但领域专精的专家模型**（如 Code Llama-Python 7B）在特定任务上可以超越通用大模型。

- **定义**：**协作专家模型（CoE）** 通过整合多个专家模型来共同完成任务。
- **核心价值**：
  - **超高精度**：以电路板检测（99.9%准确率）为例，说明单一模型无法达到的精度，可以通过专家协作实现。
  - **满足严苛需求**： CoE 适用于对生成结果精度要求极高的场景（如智能制造、医疗诊断）。

### 二、核心问题深度剖析

论文指出的不是单一问题，而是一个由**硬件限制**、**模型特性**和**系统设计缺陷**共同构成的**系统性瓶颈**：

1. **根本矛盾：CoE的大内存需求 vs. 边缘设备的有限内存**
   - **模型侧**：一个高质量的CoE应用（如电路板检测）需要**数百个专家模型**（论文中>300个），总参数量达数十GB（如13B参数，60GB内存）。
   - **设备侧**：典型的边缘GPU（如RTX 3080Ti）仅有**12GB显存**。
   - **结果**：专家模型无法常驻GPU，必须存放在更慢的存储层级（CPU内存或SSD）。

2. **直接瓶颈：专家切换的灾难性开销**
   - 当需要的专家不在GPU时，必须从SSD/CPU加载，这个过程称为 **“专家切换（Expert Switching）”**。
   - **关键数据**：**一次从SSD到GPU的专家切换延迟，占单次推理总延迟的90%以上**。即使是从CPU到GPU的切换，也占60%以上。这彻底颠覆了“计算是瓶颈”的常见认知，**I/O（专家切换）成为了绝对主导的性能杀手**。

3. **现有系统（如Samba-CoE）的设计缺陷**

论文精准地指出了导致低效切换的三个具体原因：

- **请求调度无视依赖**：采用简单的**先到先服务（FCFS）**。假设请求A和C都需要专家X，但请求B不需要。FCFS的执行顺序A->B->C会导致：A用完专家X后，B将其驱逐，C又必须重新加载它。**这本可以通过重排为A->C->B来避免**。

- **专家管理基于无效历史**：采用**LRU（最近最少使用）** 策略来驱逐专家。LRU只反映“过去谁用得少”，但无法预测“未来谁更需要”。在CoE中，专家的使用由路由逻辑决定，**未来使用概率是可以提前预估的**，LRU却无法利用这一信息。
- **内存分配缺乏智能**：GPU内存既要存专家参数，又要存推理时的中间结果（随着批处理大小增大而增大）。这是一个两难权衡：多存专家可以减少切换，但会限制批处理大小，降低计算利用率。现有系统没有提供自动化的、适应不同硬件的最优配置方案。

4. **被忽视的关键：专家依赖关系**
   - **请求间依赖**：如上所述，不同请求可能依赖同一个专家。
   - **专家间依赖**：在一个推理流水线中，专家B的执行可能需要专家A的输出作为输入。
   - **论文的核心洞察**：**CoE模型中这种内在的、可预知的依赖关系，是优化调度和管理的“金钥匙”**，但被现有系统完全忽略了。

### 三、CoServe 工作详细介绍

CoServe 是一个**完整的系统级解决方案**，它通过**离线分析**和**在线调度管理**相结合的方式，系统性攻克了上述问题。

**整体架构（三个阶段）**

1. **离线阶段**：为每个目标设备“体检”，自动寻找最优配置。
2. **系统初始化**：根据离线配置，创建执行器并按最优策略加载专家。
3. **在线服务阶段**：处理实时请求，执行依赖感知的调度与管理。

#### **技术一：依赖感知的请求调度**

目标：将使用相同专家的请求“凑”在一起处理，减少专家被来回加载/驱逐的次数。

- **请求分配**：一个新请求到来时，系统会预测将其加入**哪个执行器（CPU/GPU）的队列**，能使**所有队列的总完成时间最短。这实现了跨队列的负载均衡。
- **请求重排**：请求进入某个队列后，**会被插入到最后一个使用相同专家的请求后面**。这保证了依赖同一专家的请求在队列中连续排列。
- **请求批处理分割**：根据当前可用内存和性能分析器给出的“最大高效批处理大小”，将连续队列分割成合适的批次进行推理。

#### **技术二：依赖感知的专家管理**

目标：当必须驱逐专家以腾出空间时，做出“伤害最小”的选择。

- **两阶段驱逐策略：
  - **阶段1：驱逐“无前驱依赖”的专家**。有些专家必须在其依赖的专家执行完后才会被用到，它们提前占用内存是一种浪费，优先驱逐它们。
  - **阶段2：按“使用概率”驱逐**。如果阶段1后内存仍不足，则按照**离线阶段计算好的专家使用概率**，从低到高依次驱逐。这比LRU基于历史统计的预测准确得多。

#### **技术三：离线分析器与自适应内存管理**

目标：让系统能自动适应不同的硬件，找到内存分配的最佳平衡点。

- **专家性能画像**：自动测量每个专家在不同批处理大小下的**执行延迟**和**内存占用**。
- **专家使用概率计算**：通过运行一个小型真实数据集，或直接根据用户预定义的路由规则，统计出每个专家的**被调用概率**。
- **自适应内存分配搜索算法**（核心创新）：
  - **输入**：专家使用概率的累计分布函数。
  - **方法**：采用一个**衰减滑动窗口**在CDF曲线上搜索。
  - **过程**：从加载较多专家开始，逐步减少加载数量，并测试每个配置下的系统吞吐量。吞吐量会先因计算利用率提高而上升，后因内存竞争（专家参数 vs. 中间结果）而下降。
  - **输出**：找到**吞吐量开始下降的拐点窗口**，从中选择一个值作为GPU上应常驻的专家数量。剩余内存则分配给推理中间结果。

## Klotski: Efficient Mixture-of-Expert Inference via Expert-Aware Multi-Batch Pipeline

### 一、研究背景

深度学习快速发展，大语言模型（LLMs）在各种任务中表现出卓越性能。为了在不增加训练和推理成本的情况下扩展模型规模，研究者提出了**稀疏激活的Mixture-of-Experts（MoE）模型**。

### 二、MoE推理的内存挑战与卸载技术

- **内存瓶颈问题**：

  - 模型参数量与硬件内存发展不匹配。
  - 举例：DeepSeek-V2（2360亿参数）需要至少7张H100 GPU（每张80GB）。
  - 高昂内存成本限制了在个人电脑、小服务器等常见环境中的使用。

- **核心问题提出**：

  如何在GPU内存与模型参数规模存在巨大差距的资源受限环境中部署MoE模型？

- **卸载技术作为解决方案**：

  - 将当前计算不需要的张量卸载到CPU内存或磁盘。
  - MoE的稀疏激活特性使其特别适合卸载（更多参数可被卸载）。

### 三、现有卸载方法的问题分析

- **现有MoE卸载方法概述**：

  - 计算当前层时预取下一层参数，实现I/O与计算部分重叠。
  - 举例：MoE-Infinity基于专家激活轨迹进行预取；SIDA训练离线专家预测器，准确率>90%。

- **两类气泡问题详细分析**：

  **1. 层间气泡（Inter-layer bubbles）**

  - **原因**：注意力层与专家层之间的**计算-I/O**不平衡。
  - **数据支撑**：Mixtral-8x7B，batch size=16，平均注意力计算约2.6ms，单个专家传输约21ms。
  - **问题加剧**：当门控选择多个专家（如top-2）时，I/O开销成倍增加。

  **2. 层内气泡（Intra-layer bubbles）**

  - **原因**：专家层内部的计算与I/O不平衡。
  - **对比说明**：稠密模型中FFN处理整个批次；MoE中每个专家只处理部分序列。
  - **数据支撑**：Mixtral-8x7B中处理一个令牌的单个专家计算<1ms，远小于传输延迟。

### 四、多批次计算的思路及其局限性

- **从稠密模型获得的启发**：
  - 相关研究（如FlexGen）提出同时考虑多个批次的计算，延长计算时间以覆盖下一层I/O。
  - 权重在多个批次间共享，减少层间气泡。
- **在MoE中应用此思路的挑战**：
  - 多批次增加MoE层输入的多样性。
  - 门控机制对数据变化敏感，激活专家总数可能增加。
  - **关键洞察**：虽然注意力层的多次计算可以重叠部分专家的I/O，但更多专家被激活会导致**更多层内气泡**。

### 五、本文工作

KLOTSKI是一个**面向MoE模型的高效推理引擎**，通过在资源受限环境中实现**近乎零气泡的推理管道**，显著提升吞吐量。

1. **核心创新：专家感知的多批次流水线范式（Expert-Aware Multi-Batch Pipeline）**

- **多批次计算共享权重**：将多个批次的注意力层计算时间延长，以覆盖下一层参数的加载时间，减少层间气泡。
- **专家感知的调度**：
  - **热专家（Hot Experts）与冷专家（Cold Experts）识别**：基于观察发现，在MoE推理中，少数专家处理大多数令牌，称为“热专家”，其计算需求高、I/O需求低；其余为“冷专家”。
  - **计算顺序重排**：优先调度热专家的计算，同时并行加载冷专家的参数，实现计算与I/O的重叠，减少层内气泡。

2. **关键组件设计**

   a) **约束敏感的I/O-计算规划器（Constraint-Sensitive I/O-Compute Planner）**

- 根据硬件环境（GPU/CPU/磁盘带宽、内存容量）和模型结构，动态规划最优的批次大小 n*n* 和计算顺序，以最小化气泡时间 Tb*T**b*。

- 通过解一组不等式（如公式7）确定最小的 n*n*，使得计算时间能覆盖I/O时间。

  b) **相关性感知的专家预取器（Correlation-Aware Expert Prefetcher）**

- 构建**专家相关性表**，基于令牌在前几层的专家选择历史，预测当前层可能激活的热专家。

- 仅预取**门控（gate）和热专家**，而非整个MoE层，减少不必要的I/O。

  c) **自适应张量放置（Adaptive Tensor Placement）**

- 在多级存储系统（VRAM → DRAM → Disk）中智能分配模型张量。

- 优先将专家参数放在CPU内存中，利用其较高带宽实现快速加载。

# arkiv

## PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC

### 一、研究背景

- **MLLM（多模态大语言模型）的崛起**：近年来，MLLM在多个领域取得显著进展，尤其是在**视觉-语言理解与推理**方面。
- **GUI智能体的兴起**：基于MLLM的智能体被扩展到**图形用户界面（GUI）自动化**领域，用于操作智能手机、PC等设备，具有广泛的应用潜力。

### 二、PC场景的独特挑战

作者指出，与智能手机相比，**PC环境更为复杂**，主要体现在以下两个方面：

1. 更复杂的交互环境

- PC界面包含**更密集、更多样化的交互元素**（如图标、控件），且**缺乏明确的文本标签**说明其功能。
- 文本布局复杂多样（如Word文档、VS Code代码）。
- **现有MLLM在PC屏幕感知能力不足**：即使是当前最先进的MLLM（如Claude-3.5），在GUI grounding任务上的准确率仅为**24.0%**（如图1(a)所示）。

> **举例**：Word顶部的工具栏有很多图标，但没有文字说明其功能，这给MLLM的感知和理解带来困难。

2. 更复杂的任务序列

- PC常用于生产力场景，涉及**多个应用内（intra-app）和应用间（inter-app）的工作流**。
- 任务步骤更长、更复杂，且**子任务之间存在依赖关系**。
- **指令级成功率远低于子任务级成功率**：如图1(b)所示，单智能体（GPT-4o）在子任务级成功率为41.8%，但在指令级成功率骤降至8%。

> **举例**：制定旅行计划可能涉及Chrome搜索、Word记录、Excel整理、日历设置等多个应用，共28个步骤，且后续任务依赖前序任务的结果。

### 三、现有方法的不足

**1. UFO（Zhang et al., 2024）**

- 采用**双智能体框架**：一个选择应用，一个执行操作。
- **问题**：缺乏对**屏幕文本的细粒度感知和操作能力**，无法处理Word文档编辑等需要精确文本操作的任务。

**2. Agent-S（Agashe et al., 2024）**

- 结合在线搜索和本地记忆进行**经验增强规划**。
- **问题**：同样**忽略子任务间的复杂依赖关系**，在真实跨应用任务中表现有限。

### 四、本文提出的解决方案

为了应对上述挑战，本文提出 **PC-Agent框架**，包含三个核心设计：

**1. 主动感知模块（Active Perception Module, APM）**

- 提升对**交互元素和文本的细粒度感知与操作能力**。
- 交互元素：使用无障碍树提取位置和含义。
- 文本操作：使用MLLM理解用户意图 + OCR精确定位目标文本。

**2. 分层多智能体协作架构（Hierarchical Multi-agent Collaboration）**

- 将决策过程分解为三层：**指令 → 子任务 → 动作**。
- 设置三个智能体：
  - **Manager Agent（MA）**：分解指令，管理子任务依赖。
  - **Progress Agent（PA）**：跟踪子任务进度。
  - **Decision Agent（DA）**：执行具体操作。
- 实现**自上而下的任务分解**与**自下而上的进度反馈**。

**3. 基于反思的动态决策机制（Reflection-based Dynamic Decision-making）**

- 引入**Reflection Agent（RA）**，在执行每一步后检查屏幕变化，判断操作是否正确。
- 若出错，及时反馈给DA和PA，实现**动态纠错和调整**。

## MAPoRL2: Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning

### 一、研究背景与机遇

- **核心观点**：大语言模型（LLMs）在**多智能体框架**中展现出巨大的协作潜力。
- **现状描述**：许多研究（如多智能体辩论、角色扮演）已经表明，让多个LLM协同工作可以解决更复杂的任务。这标志着研究重点从“单个强大的LLM”转向了“多个LLM组成的智能系统”。
- **隐含价值**：多智能体系统引入了新的维度和可能性，但同时也带来了新的挑战。

### 二、核心问题与现有方法的不足

- **问题一：依赖“提示”而非“训练”**。
  - **现状**：当前的多LLM协作方法几乎完全依赖于**提示工程**来激发预训练模型的协作行为。
  - **局限**：这仅仅是在“模拟”协作流程，而不是从根本上“训练”模型学会协作。LLM在预训练时并没有以“有效合作”为目标进行优化。
- **问题二：协作效果不稳定且有限**。
  - **证据**：作者引用文献指出，多轮辩论并不总能带来性能提升。这说明现成的LLM缺乏**战略性的协作能力**。
- **根本原因分析（理论层面）**：
  - **类比**：作者用了一个精妙的比喻：在协作中，如果你的对手是“未经训练”且“非战略性”的，那么它就无法做出促进协作的行为。
  - **核心论点**：**单智能体训练不足以实现真正的协作**。真正的协作行为需要在**交互式训练环境**中产生，每个智能体必须根据其他智能体的行为动态优化自己的策略。这是一个典型的**多智能体强化学习（MARL）** 问题。
  - **补充否定**：作者也预先指出，传统的监督微调（SFT）对于学习复杂的互动策略也是不够的，因为它只是在模仿数据中的交互模式，而非学习在动态环境中进行策略性互动。

### 三、本文解决方案：MAPoRL

- **核心思想**：这是一个使用**多智能体强化学习（MARL）** 对多个LLM进行**协同训练**的新范式。
- **运作机制简述**：
  1. 在一个预设的协作框架（如辩论框架）中，每个智能体根据其回答质量和互动效果获得奖励。
  2. 每个智能体的目标是最大化自己的累积奖励（价值函数）。
  3. 通过设计特定的奖励机制，鼓励有效的互动，并对协作失败施加惩罚，从而引导LLM产生更协同、更高效的行为。

## OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation(NeurIPS'2025 accept)

### 一、研究背景

- 大型语言模型（LLM）已经从**纯文本预测器**发展为能做“规划、调用工具、多步推理”的**自主 agent**，并且出现了**多智能体系统（MAS）**，通过把复杂任务拆给不同专长的 agent 来提升能力。

- 强调从“一个大模型做一切”到“多个专长 agent 协同”的范式转变。

### 二、现有多智能体方法的两大短板

论文指出现有 MAS 在**跨领域迁移（cross-domain transferability）**上有严重不足，表现为两个方面的限制：

1. **推理/部署侧的问题（Inference）**：把系统用到新领域通常需要**完全重设计**（例如 MetaGPT 偏向软件工程），导致不能快速复用。
2. **训练侧的问题（Training）**：许多方法需要对每个 agent 都做专门优化/微调（例如 MALT 的 generator/verifier/refiner 流程），迁移到新领域往往要**重新训练整套 agent**，代价很高。

**Tips:**

- 推理端需重设计意味着工程成本高、维护复杂、难以扩展到新的业务场景。
- 训练端需重训意味着计算成本高、容易过拟合到某套工具/流程，降低泛化能力。
  这两点合起来限制了 MAS 在现实工业/多领域场景的可用性。

### 三、本文方法

作者提出 **WORKFORCE**：一个**分层且模块化**的多智能体推理框架，**关键思想是把“战略性规划（planning）”与“领域执行（execution）”分离**。框架由三大组件组成：

- **Domain-agnostic Planner（领域无关的规划器）**：负责把高层目标拆成抽象子任务（task decomposition）。
- **Coordinator（协调器）**：负责分配子任务并管理依赖与中间结果。
- **Domain-specific Worker Nodes（领域专用的 worker）**：带有具体工具调用能力的执行单元。
  由于这种解耦，WORKFORCE 能在推理端做到 **plug-and-play**：遇到新领域只需加入或替换 worker，而不改 planner。

**Tips:**

- 假设存在“通用的规划能力”可以覆盖不同领域的任务分解模式；只要 planner 给出合适的抽象子任务，领域 worker 就能用各自工具完成。
- 这种设计把复杂性分层：Planner 专注高层策略，Workers 专注低层执行细节，便于工程化与模块化维护。

### 四、训练方法

在训练层面，作者提出 **OWL（Optimized Workforce Learning）**，核心思想是：**只重点训练/优化领域无关的 Planner**，而让 Worker 保持“可插拔”的工具集合。训练流程为 **两阶段**：

1. **SFT（监督微调）**：用专家示例初始化 planner 的基本分解技能。
2. **RL（强化学习 / DPO）**：用偏好优化（Direct Preference Optimization, DPO）等方法基于真实/合成反馈提升 planner 的泛化与决策质量。

**Tips:**

- 只训练 planner 大幅降低训练开销（训练规模小、成本低）。
- 通过 RL 增强 planner 在看不到标注或跨域情形下的策略选择能力，减少对任务特定示例的依赖。
- 避免将整个系统（包含工具调用细节）一起微调，从而降低对具体工具/环境的耦合。

## Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies（ICML '25 reject）

### 一、 研究背景

肯定了LLM在理解和推理方面的强大能力，并指出**基于LLM的智能体（Agent）** 已成为处理复杂任务的重要范式。这些智能体通过**提示词（Prompts）** 进行“编程”，使其能够使用工具、与环境交互，并在多轮对话中完成任务。应用领域广泛，包括：

- **代码生成与调试**
- **检索增强生成（RAG）**
- **数据分析**
- **交互式决策**

作者进一步指出，将多个LLM智能体按照特定**拓扑结构（Topologies）** 组织起来，形成一个协作系统，往往能超越单智能体。这是因为MAS能整合**多元的视角和角色**（例如，有些智能体负责生成，有些负责验证，有些负责辩论），从而提升任务解决能力。

### 二、核心问题

然而，为特定领域**设计高效的MAS非常困难**。有两大核心挑战：

- **1. 提示词敏感性（Prompt Sensitivity）**：
  - 单个LLM智能体对提示词的微小改动非常敏感，可能导致性能大幅波动。
  - 在MAS中，当多个敏感的智能体串联（Cascade）时，这种不稳定性会被**级联放大**，使得系统表现难以预测和优化。
- **2. 拓扑结构设计的复杂性**：
  - 设计有效的协作拓扑（如哪些智能体以何种顺序连接）需要大量**试错性的人工实验**。
  - 整个设计空间是**组合爆炸**的：既要搜索**近乎无限的提示词设计空间**，又要决定**在拓扑中集成哪些类型的智能体**。

### 三、现有工作的局限

现有自动化方法存在不足：

- **DSPy**：专注于优化提示词中的示例，而非整个MAS。
- **Li et al. 等工作**：主要通过增加投票智能体的数量来优化，方法相对单一。
- **ADAS, AFlow**：尝试自动化拓扑设计，但**严重忽略了提示词的联合优化**。
- **共同问题**：缺乏对**提示词设计和拓扑设计之间相互作用**的深入理解和系统优化。现有工作要么只优化拓扑，要么只优化提示词，未能将两者视为一个需要**协同优化的整体**。

### 四、研究路径与核心贡献

1. **首先，进行深度分析**：系统地探究MAS设计空间中各因素（如提示词优化、智能体数量缩放、不同拓扑）对性能的影响。
2. **然后，基于洞察构建框架**：分析发现，**高质量的提示词是高性能MAS的基础**，且**仅有少数拓扑结构真正有效**。因此，可以将搜索重点集中在这些“有影响力”的组件上，从而**修剪搜索空间，降低优化复杂度**。
3. **最终，提出Mass框架**：这是一个**新颖的多阶段优化框架**，通过**交错进行（Interleaving）** 从局部到全局、从提示词到拓扑的优化，自动化地设计MAS。
   - **阶段一（块级提示优化）**：先“预热”优化每个基础智能体模块的提示词。
   - **阶段二（工作流拓扑优化）**：在修剪后的拓扑空间中，基于优化好的智能体模块，搜索最优协作结构。
   - **阶段三（工作流级提示优化）**：在找到的最优拓扑上，进行全局的提示词微调，以优化智能体间的协同。

## Multi-agent Architecture Search via Agentic Supernet(ICML '25 Oral)

### 一、研究背景

**1. 单智能体系统的成功**

- **起点**：基于大语言模型的智能体（LLM-based agents）已在**问答、数据分析、代码生成、网页导航、数据合成**等领域取得显著进展。
- **赋能手段**：通过赋予LLM**角色设定、工具调用、规划能力、记忆机制**等，增强了其任务执行能力。

**2. 多智能体系统的崛起**

- **发现**：将多个LLM智能体组合成**多智能体系统**，能进一步突破单智能体的认知边界，展现出**集体智能**。
- **方式**：既可通过**协作**（如合作求解），也可通过**竞争**（如辩论）来提升系统整体能力。

### 二、现有系统的局限

**1. 早期系统：依赖手工设计**

- **代表工作**：CAMEL、AutoGen、MetaGPT 等。
- **问题**：严重依赖**人工配置**（如提示工程、角色设定、通信流程），导致：
  - 难以快速适应新领域
  - 扩展性和灵活性差

**2. 当前自动化方法：仍存根本缺陷**

- **研究方向**：社区开始尝试**自动化设计**，分为三类：
  - **提示优化**（如 DsPy、EvoPrompting）
  - **智能体间通信优化**（如 GPTSwarm、G-Designer）
  - **智能体角色优化**（如 EvoAgent、AutoAgents）
- **更高阶方法**：ADAS、AgentSquare、AFlow 等尝试**搜索整个工作流程**。
- **根本缺陷**：它们仍遵循 **“寻找单一最优系统”** 的范式。

### 三、核心问题

#### 困境1：资源分配僵化

- 现有方法倾向于设计一个**复杂且资源密集**的系统（例如包含数十次LLM调用和工具使用）。
- 但**并非所有任务都需要如此复杂的系统**：
  - **博士级抽象代数**可能需要复杂系统
  - **小学算术**可能只需简单零样本I/O
- 结果：系统在简单任务上**过度消耗资源**。

#### 困境2：跨领域适应性差

- 不同任务领域可能需要**截然不同的系统结构**。
- **举例**：GAIA 基准测试中，**文件阅读任务**和**网页搜索任务**的最优系统可能完全不同。
- 现有方法只能**分任务优化**，无法用一个系统兼顾所有。

> **结论**：当前自动化优化单一多智能体架构的范式，**无法满足动态、演化的智能体部署需求**。

### 四、本文解决方案

**1. 范式转变**

- **从**：寻找一个（可能不存在的）单一最优系统
- **转向**：优化一个**多智能体架构的概率分布**（即**智能体超网络**）

**2. 智能体超网络的定义**

- 它是一个**概率性、连续的架构分布**，涵盖大量可能的多智能体候选结构。
- 可视为一个**多层工作流**，包含：
  - 多个智能体算子（如CoT、辩论、ReAct）
  - 每层算子的参数化概率分布

 **3. MaAS 工作机制**

- **训练阶段**：
  - 使用**控制器网络**根据输入查询采样架构。
  - 根据环境反馈更新分布参数和算子：
    - 分布参数 → 蒙特卡洛采样估计梯度
    - 算子 → 文本梯度估计更新
- **推断阶段**：
  - 针对不同查询，采样出**性能满意且资源合适**的多智能体系统。
  - 实现**任务定制化的集体智能**。

## MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework(ICML '24 Oral)

### 一、研究背景与机遇

**大语言模型与自主智能体的兴起**：

- LLM 驱动的自主智能体显示出模拟和增强人类工作流程的巨大潜力，特别是在自动化编程与任务协作方面。
- 现有系统（如 [24, 25, 26, 27, 28] 等）已能完成简单的对话与任务，但在面对**复杂、结构化、需深度协作**的问题时表现不足。

### 二、现有系统的核心问题

1. **协作效率低下**：

   - 现有系统多采用**自由对话式协作**，智能体之间缺乏明确分工与工作流，容易陷入无效交流（如“你好吗？”这样的闲聊）。

   - 缺乏**结构化输出**，导致信息传递模糊、逻辑不一致。

2. **“链式幻觉”问题**：

   - 当多个 LLM 链式协作时，前一个模型的错误或模糊输出会被后续模型放大，最终导致整体输出严重偏离目标。

3. **缺乏人类工作流程的借鉴**：

   - 现实中人类团队通过 **Standardized Operating Procedures** 高效协作，但现有智能体系统未系统性地引入此类机制。

### 三、人类工作流程的启发：SOPs

- **SOPs 在现实协作中的作用**：
  - 在软件工程、医疗、制造等领域，SOPs 通过明确**角色分工、输出标准、工作顺序**来确保任务高质量完成。
  - 例如：在软件公司中，产品经理撰写 PRD，架构师进行系统设计，工程师编写代码，测试工程师进行验证——每个角色都有明确的输出标准。
- **SOPs 的核心价值**：
  - **任务分解**：将复杂任务拆解为子任务。
  - **责任明确**：每个角色知道该做什么、输出什么。
  - **质量可控**：通过中间产物的标准化确保最终输出质量。

### 四、MetaGPT 的核心思想

1. **将 SOPs 编码为提示序列**：
   - 为每个角色（如产品经理、架构师、工程师等）设计结构化提示，要求其输出符合特定格式的文档（如 PRD、设计图、接口定义等）。
   - 通过这些结构化输出，实现信息在智能体之间的**精准传递**。
2. **模拟软件公司的工作流**：
   - MetaGPT 模拟一个虚拟软件公司，智能体扮演不同角色，按照 SOPs 顺序协作。
   - 例如：
     - 产品经理分析需求 → 输出 PRD
     - 架构师基于 PRD 设计系统 → 输出系统设计图
     - 工程师基于设计编写代码 → 输出可执行代码
     - 测试工程师验证代码 → 输出测试报告
3. **结构化通信与共享机制**：
   - 使用**文档与图表**代替自然语言对话，避免信息失真。
   - 引入**发布-订阅模式**与**共享消息池**，提高通信效率。

### 五、MetaGPT 的定位：元编程框架

**MetaGPT 属于“元编程”范式**：

- 不同于“元学习”或“学会学习”，MetaGPT 强调“**通过编程来实现编程**”，即通过智能体协作自动完成从需求到代码的全流程。
- 与之类似的工作包括 CodeBERT、CodeLlama 等，但 MetaGPT 更强调**多角色、结构化、流程化**的协作。

## Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts(MLSys '25)

### 一、研究背景与动机

1. **大语言模型的发展**

- 近年来，大语言模型（LLMs）在自然语言处理、计算机视觉和多模态感知等领域取得了革命性进展。
- **模型规模的扩大**（参数量的增加）被证明是提升模型能力的关键途径。
- 然而，**模型参数量的增长也带来了部署上的巨大挑战**，尤其是计算资源的限制。

2. **Mixture-of-Experts（MoE）的引入**

- MoE 是一种**稀疏模型架构**，通过激活部分参数来处理每个输入，从而在保持计算成本不变的前提下，支持万亿参数规模的模型。
- 例如，Mixtral 8×7B 模型有 **450亿参数**，但运行时仅激活 **140亿参数**。
- MoE 已成为扩展大模型规模的**关键架构**。

### 二、分布式MoE的通信瓶颈

1. **专家分布导致通信需求**

- 由于单个GPU无法存储所有专家，专家通常**分布在多个GPU上**。
- 在MoE层执行过程中，需要进行大量的**设备间数据交换**。

2. **通信开销占比**

- 实验数据显示，在多个主流MoE模型中，**设备间通信时间平均占整个模型执行时间的47%**（图1(a)）。
- 这成为MoE模型在分布式环境下性能的**主要瓶颈**。

### 三、现有方法的不足

1. **粗粒度流水线重叠**

- 现有方法尝试通过**流水线执行**来重叠通信和计算，即将输入数据分块，使通信和计算阶段交替进行。
- 这种方法虽然能减少整体执行时间，但存在**两个主要低效问题**：

**问题一：计算效率下降**

- 数据分块后，每个专家处理的数据量变小，导致**GPU计算资源利用率降低**。
- 分块后的总计算时间可能**超过未分块时的计算时间**。

**问题二：重叠不充分**

- 在通信的开始和结束阶段，如接收第一个数据块和发送最后一个数据块时，**无法与计算重叠**，形成“管道气泡”。
- 由于MoE中**数据依赖复杂**，难以在细粒度上实现高效重叠。

### 四、MoE的动态性与硬件挑战

1. **动态路由导致负载不均衡**

- MoE中的**令牌路由是动态的**，每个专家接收的输入形状在运行时变化。
- 这导致不同GPU上的**通信和计算负载不均衡**。

2. **现有内核调度方法的限制**

- 现有研究通常将通信和计算任务封装到**不同的内核中**，运行在不同的流上。
- 这种方法**限制了硬件资源的控制**，导致内核性能不稳定，难以实现无缝重叠。

### 五、本文的贡献：Comet系统

1. **核心思想**

- 提出 **Comet**，一个**面向MoE的细粒度通信-计算重叠系统**。
- 通过**数据依赖分析和任务重调度**，实现通信和计算在**细粒度上的精确重叠**。

2. **关键技术**

- **基于共享张量的依赖解析**：
  - 分析MoE层中通信和计算之间的数据依赖关系。
  - 通过**分解共享张量并重调度计算**，实现更高效的重叠。

- **自适应负载分配**：
  - 根据输入形状、模型配置和硬件环境，**动态分配GPU资源**给通信和计算任务。
  - 生成**高效的融合内核**，最大化延迟隐藏。
